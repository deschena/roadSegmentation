{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5220,
     "status": "ok",
     "timestamp": 1608042891382,
     "user": {
      "displayName": "Germain Fragnière",
      "photoUrl": "",
      "userId": "11075373030229809982"
     },
     "user_tz": -60
    },
    "id": "jEr0AdMJzhw6",
    "outputId": "4fc3ab90-e0a7-440b-ebb8-7b0c7d7a2a32"
   },
   "outputs": [],
   "source": [
    "!ssh-keygen -t rsa -b 4096\r\n",
    "!ssh-keyscan -t rsa github.com >> ~/.ssh/known_hosts\r\n",
    "!cat /root/.ssh/id_rsa.pub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 103711,
     "status": "ok",
     "timestamp": 1608043011039,
     "user": {
      "displayName": "Germain Fragnière",
      "photoUrl": "",
      "userId": "11075373030229809982"
     },
     "user_tz": -60
    },
    "id": "O-_ODVM9z0TI",
    "outputId": "46a0c49c-2fb9-485e-ee35-a47fd941af9a"
   },
   "outputs": [],
   "source": [
    "!ssh -T git@github.com\r\n",
    "!git config --global user.email \"justin.deschenauxy@epfl.com\"\r\n",
    "!git config --global user.name \"Justin-Collab\"\r\n",
    "!git clone git@github.com:deschena/colab_unet_train.git\r\n",
    "!mv colab_unet_train/* .\r\n",
    "from google.colab import drive\r\n",
    "drive.mount('/content/gdrive')\r\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1006,
     "status": "ok",
     "timestamp": 1608043406728,
     "user": {
      "displayName": "Germain Fragnière",
      "photoUrl": "",
      "userId": "11075373030229809982"
     },
     "user_tz": -60
    },
    "id": "wkXuMSgTzd7A"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import os, sys, io, random\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n",
    "\n",
    "from datasets.AugmDataset import AugmDataset\n",
    "from models.Unet import Unet\n",
    "from models.DenseUnet import DenseUnet\n",
    "from utils import *\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qpWNkB_czd7H"
   },
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1000,
     "status": "ok",
     "timestamp": 1608043406732,
     "user": {
      "displayName": "Germain Fragnière",
      "photoUrl": "",
      "userId": "11075373030229809982"
     },
     "user_tz": -60
    },
    "id": "znv4FZwlzd7J"
   },
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "root_path = \"datasets/augmented_dataset/\"\n",
    "train_name = \"msel_train/\"\n",
    "valid_name = \"msel_valid/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 998,
     "status": "ok",
     "timestamp": 1608043406734,
     "user": {
      "displayName": "Germain Fragnière",
      "photoUrl": "",
      "userId": "11075373030229809982"
     },
     "user_tz": -60
    },
    "id": "cWjonsh2zd7K"
   },
   "outputs": [],
   "source": [
    "def train_net(net, train_name, valid_name, seed=999, max_epoch=50, net_name=\"DEFAULT\", patience=5, verbose=True, batch_size=4):\n",
    "    torch.random.manual_seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    root_path = \"datasets/augmented_dataset/\"\n",
    "    \n",
    "    # Since we had the best results with only the binary cross entropy, we combine the final sigmoïd \n",
    "    # activation with the loss, since that way we have a numerically more stable result, as the \n",
    "    # log-sum-exp trick is used.\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    \n",
    "    train_set = AugmDataset(root_dir=root_path, name=train_name)\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=batch_size)    \n",
    "\n",
    "    validation_set = AugmDataset(root_dir=root_path,name=valid_name)\n",
    "    validation_loader = DataLoader(validation_set, batch_size=2*batch_size, shuffle=False, num_workers=2*batch_size)\n",
    "    \n",
    "    # Send to GPU, prepare optimizer and learning rate scheduler\n",
    "    net.to(device)\n",
    "    optimizer = optim.Adam(net.parameters())\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=patience, verbose=verbose)\n",
    "    \n",
    "    validation_loss = []\n",
    "    training_loss = []\n",
    "    loss = -1\n",
    "    best_current_loss = -1\n",
    "    \n",
    "    for epoch in range(max_epoch):\n",
    "        net.train()\n",
    "        for batch_train, batch_gt in train_loader:\n",
    "            \n",
    "            # Send data to gpu\n",
    "            batch_train = batch_train.to(device)\n",
    "            batch_gt = batch_gt.to(device)\n",
    "            \n",
    "            # Clear accumulated gradients & compute prediction\n",
    "            optimizer.zero_grad()\n",
    "            output = net(batch_train)\n",
    "            # Compute loss, gradient & update parameters\n",
    "            loss = criterion(output, batch_gt)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # After each epoch, compute & save loss on training and validation sets\n",
    "        v_perf = validation_perf(net, validation_loader)\n",
    "        validation_loss.append(v_perf)\n",
    "        training_loss.append(loss)\n",
    "        # Check if scheduler must decrease learning rate\n",
    "        scheduler.step(v_perf)\n",
    "        if v_perf > best_current_loss:\n",
    "            # Save best net\n",
    "            torch.save(net.state_dict(), f\"/content/gdrive/My Drive/ML files/model_selection/{net_name}.pth\")\n",
    "            v_perf = best_current_loss\n",
    "        if verbose and epoch % 10 == 0:\n",
    "            print(f\"{epoch} epochs elapsed\")\n",
    "            \n",
    "    return training_loss, validation_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vB2YS_cwzd7L"
   },
   "source": [
    "## Train the models\n",
    "**Models considered**:\n",
    "1. Standard Unet\n",
    "2. Attention Unet (channel attention)\n",
    "3. Attention Unet (pixel attention)\n",
    "4. Dense Unet\n",
    "5. Dense Attention Unet (channel attention)\n",
    "6. Dense Attention Unet (pixel attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note**: The last layer of the sigmoid is deactivated during training because it is included in the loss, indeed, it yields a more stable function by leveraging the \"log-sum-exp\" trick. When the model is in eval mode, or activation_output is True, the last layer is there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4002201,
     "status": "ok",
     "timestamp": 1608047410341,
     "user": {
      "displayName": "Germain Fragnière",
      "photoUrl": "",
      "userId": "11075373030229809982"
     },
     "user_tz": -60
    },
    "id": "KqGuF7M5zd7L",
    "outputId": "ce7c46f5-160b-496c-a032-8e2a6d63b6f9"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "net1 = Unet(activation_output=False)\n",
    "net1_tr, net1_val = train_net(net1, train_name, valid_name, net_name=\"unet\", seed=123123)\n",
    "np.save(f\"/content/gdrive/My Drive/ML files/model_selection/net1_tr_loss\", net1_tr)\n",
    "np.save(f\"/content/gdrive/My Drive/ML files/model_selection/net1_val_loss\", net1_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8113642,
     "status": "ok",
     "timestamp": 1608051522110,
     "user": {
      "displayName": "Germain Fragnière",
      "photoUrl": "",
      "userId": "11075373030229809982"
     },
     "user_tz": -60
    },
    "id": "xyLx00cKzd7M",
    "outputId": "4948cf6a-900b-42d5-c7ca-3beb377a9625"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "net2 = Unet(attention=\"channel\", activation_output=False)\n",
    "net2_tr, net2_val = train_net(net2, train_name, valid_name, net_name=\"channel_unet\", seed=4325443)\n",
    "np.save(f\"/content/gdrive/My Drive/ML files/model_selection/net2_tr_loss\", net2_tr)\n",
    "np.save(f\"/content/gdrive/My Drive/ML files/model_selection/net2_val_loss\", net2_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12601652,
     "status": "ok",
     "timestamp": 1608056010387,
     "user": {
      "displayName": "Germain Fragnière",
      "photoUrl": "",
      "userId": "11075373030229809982"
     },
     "user_tz": -60
    },
    "id": "64GwSSTTzd7M",
    "outputId": "80cc84b0-b7c1-4ece-cc62-57a82a92bdc8"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "net3 = Unet(attention=\"grid\", activation_output=False)\n",
    "net3_tr, net3_val = train_net(net3, train_name, valid_name, net_name=\"grid_unet\", seed=989873)\n",
    "np.save(f\"/content/gdrive/My Drive/ML files/model_selection/net3_tr_loss\", net3_tr)\n",
    "np.save(f\"/content/gdrive/My Drive/ML files/model_selection/net3_val_loss\", net3_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qdXUVLXrzd7N"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "net4 = DenseUnet(down_config=(4, 8, 16, 32), bottom=64, up_channels=(256, 128, 64, 32), activation_output=False)\n",
    "net4_tr, net4_val = train_net(net4, train_name, valid_name, net_name=\"dense_unet\", seed=776834)\n",
    "np.save(f\"/content/gdrive/My Drive/ML files/model_selection/net4_tr_loss\", net4_tr)\n",
    "np.save(f\"/content/gdrive/My Drive/ML files/model_selection/net4_val_loss\", net4_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RS2ZHGEwzd7O"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "net5 = DenseUnet(down_config=(4, 8, 16, 32), bottom=64, up_channels=(256, 128, 64, 32), activation_output=False, attention=\"channel\")\n",
    "net5_tr, net5_val = train_net(net5, train_name, valid_name, net_name=\"dense_channel_unet\", seed=445366)\n",
    "np.save(f\"/content/gdrive/My Drive/ML files/model_selection/net5_tr_loss\", net5_tr)\n",
    "np.save(f\"/content/gdrive/My Drive/ML files/model_selection/net5_val_loss\", net5_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y_Ua2FDqzd7P",
    "outputId": "2465df3d-3277-4a9e-c5c9-d7797edfb5c7"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "net6 = DenseUnet(down_config=(4, 8, 16, 32), bottom=64, up_channels=(256, 128, 64, 32), activation_output=False, attention=\"grid\")\n",
    "net6_tr, net6_val = train_net(net6, train_name, valid_name, net_name=\"dense_grid_unet\", seed=445366)\n",
    "np.save(f\"/content/gdrive/My Drive/ML files/model_selection/net6_tr_loss\", net6_tr)\n",
    "np.save(f\"/content/gdrive/My Drive/ML files/model_selection/net6_val_loss\", net6_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the performance of each architecture\n",
    "After training those 6 U-nets, we wanted to evaluate their performance on the test set (20% of the original data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_net_params(net, name):\n",
    "    path = \"experiments/model_selection/\" + name + \".pth\"\n",
    "    params = torch.load(path)\n",
    "    net.load_state_dict(params)\n",
    "    net.eval()\n",
    "    net.to(\"cuda\")\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perf_on_test_set(net, dataset_path, dataset_name):\n",
    "    dataset = AugmDataset(root_dir=dataset_path,name=dataset_name)\n",
    "    loader = DataLoader(dataset, batch_size=8, shuffle=False, num_workers=8)\n",
    "    res = validation_perf(net, loader)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net1 = Unet(activation_output=True) # This time we don't combine it with the loss, so we want to have the last activation\n",
    "net1 = load_net_params(net1, \"unet\")\n",
    "print(perf_on_test_set(net1, \"datasets/augmented_dataset/\", \"msel_test/\"))\n",
    "del net1 # To avoid filling the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = Unet(attention=\"channel\", activation_output=True)\n",
    "net2 = load_net_params(net2, \"channel_unet\")\n",
    "print(perf_on_test_set(net2, \"datasets/augmented_dataset/\", \"msel_test/\"))\n",
    "del net2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net3 = Unet(attention=\"grid\", activation_output=True)\n",
    "net3 = load_net_params(net3, \"grid_unet\")\n",
    "print(perf_on_test_set(net3, \"datasets/augmented_dataset/\", \"msel_test/\"))\n",
    "del net3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net4 = DenseUnet(down_config=(4, 8, 16, 32), bottom=64, up_channels=(256, 128, 64, 32), activation_output=True)\n",
    "net4 = load_net_params(net4, \"dense_unet\")\n",
    "print(perf_on_test_set(net4, \"datasets/augmented_dataset/\", \"msel_test/\"))\n",
    "del net4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net5 = DenseUnet(down_config=(4, 8, 16, 32), bottom=64, up_channels=(256, 128, 64, 32), activation_output=True, attention=\"channel\")\n",
    "net5 = load_net_params(net5, \"dense_channel_unet\")\n",
    "print(perf_on_test_set(net5, \"datasets/augmented_dataset/\", \"msel_test/\"))\n",
    "del net5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net6 = DenseUnet(down_config=(4, 8, 16, 32), bottom=64, up_channels=(256, 128, 64, 32), activation_output=True, attention=\"grid\")\n",
    "net6 = load_net_params(net6, \"dense_grid_unet\")\n",
    "print(perf_on_test_set(net6, \"datasets/augmented_dataset/\", \"msel_test/\"))\n",
    "del net5"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "test_architectures.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
